{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For understanding, how pyspark SQL operates, we use \n",
    "# https://www.codementor.io/jadianes/python-spark-sql-dataframes-du107w74i\n",
    "# for help\n",
    "# it worked once, so we could read the 'sheet.csv'-file which contains 36 rows of the typical taxidata\n",
    "# due to massive problems with the python kernel on juptyer, it was not possible to replicate the results for some screenshots\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "config = SparkConf().setAppName('test-topic')\n",
    "sc = SparkContext(conf=config)\n",
    "#ssc = StreamingContext(\"local[2]\", 1) # create streaming context\n",
    "#raw_data = KafkaUtils.createDirectStream(ssc, ['test-topic'], { \"metadata.broker.list\": \"localhost:9092\"})\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "#extr_data = raw_data.map(lambda fu: fu[1])\n",
    "#extr_data.pprint(20)\n",
    "#print(extr_data)\n",
    "extr_data = sc.read.csv(\"hdfs://loudacre/sheet.csv\").cache()\n",
    "#extr_data.show()\n",
    "csv_data = extr_data.map(lambda l: l.split(\",\")).zipWithIndex().filter(lambda (row,index): index > 1).keys()\n",
    "print(csv_dataghhgj7865,\n",
    "      ,\n",
    "row_data = csv_data.map(lambda p: Row(\n",
    "        VendorID=int(p[0]), \n",
    "        tpep_pickup_datetime=p[1],\n",
    "        tpep_dropoff_datetime=p[2],\n",
    "        passenger_count=int(p[3]),\n",
    "        trip_distance=float(p[4]),\n",
    "        ratecodeID=int(p[5]),\n",
    "        store_and_fwd_flag=str(p[6]),\n",
    "        pulocationid=int(p[7]),\n",
    "        dolocationid=int(p[8]),\n",
    "        payment_type=int(p[9]),\n",
    "        fare_amount=float(p[10]),\n",
    "        extra=float(p[11]),\n",
    "        mta_tax=float(p[12]),\n",
    "        tip_amount=float(p[13]),\n",
    "        tolls_amount=float(p[14]),\n",
    "        improvement_surcharge=float(p[15]),\n",
    "        total_amount=float(p[16])\n",
    "    )\n",
    ")\n",
    "\n",
    "taxi_df= sqlContext.createDataFrame(row_data)\n",
    "taxi_df.registerTempTable(\"taxi\")\n",
    "\n",
    "taxi_test= sqlContext.sql(\"\"\"\n",
    "    SELECT pulocationid, avg(tip_amount) as Average_tip_amount FROM taxi WHERE payment_type = '1' GROUP BY pulocationid\n",
    "\"\"\")\n",
    "\n",
    "taxi_test.show()\n",
    "\n",
    "#ssc.start()\n",
    "#ssc.awaitTermination()\n",
    "#ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#just some shitty\n",
    "import sys\n",
    "!sudo pyspark --packages com.databricks:spark-csv_2.10:1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# our different approach to read the data via Spark, but there we couldn't load the package 'com.databricks.spark.csv'\n",
    "# https://github.com/databricks/spark-csv as a reference\n",
    "# spark-shell --packages com.databricks:spark-csv_2.1xxxx\n",
    "# and \n",
    "# spark-submit --packages com.databricks:spark-csv_2.1 didn't work out, like manual says.\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "customSchema = StructType([ \\\n",
    "    StructField(\"VendorID\", IntegerType(), True), \\\n",
    "    StructField(\"tpep_pickup_datetime\", StringType(), True), \\\n",
    "    StructField(\"tpep_dropoff_datetime\", StringType(), True), \\\n",
    "    StructField(\"passenger_count\", IntegerType(), True), \\\n",
    "    StructField(\"trip_distance\", FloatType(), True), \\\n",
    "    StructField(\"RatecodeID\", IntegerType(), True), \\\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True), \\\n",
    "    StructField(\"PULocationID\", IntegerType(), True), \\\n",
    "    StructField(\"DOLocationID\", IntegerType(), True), \\\n",
    "    StructField(\"payment_type\", IntegerType(), True), \\\n",
    "    StructField(\"fare_amount\", FloatType(), True), \\\n",
    "    StructField(\"extra\", FloatType(), True), \\\n",
    "    StructField(\"mta_tax\", FloatType(), True), \\\n",
    "    StructField(\"tip_amount\", FloatType(), True), \\\n",
    "    StructField(\"tolls_amount\", FloatType(), True), \\\n",
    "    StructField(\"improvement_surcharge\", FloatType(), True), \\\n",
    "    StructField(\"total_amount\", FloatType(), True)])\n",
    "\n",
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(header='true') \\\n",
    "    .load('Desktop/sheet.csv', schema = customSchema)\n",
    "\n",
    "df.select('PULocationID', 'tip_amount', 'payment_type').write \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .save('Desktop/was.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "SUBMIT_ARGS = \"--packages com.databricks:spark-csv_2.11:1.2.0 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$SPARK_HOME/python/:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!whereis python2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kafka\n",
    "from kafka import SimpleProducer, SimpleClient\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "TOPIC = \"test-topic\"\n",
    "\n",
    "sc = SparkContext(\"local[2]\",\"Streamer\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# To send messages synchronously\n",
    "client = SimpleClient('localhost:9092')\n",
    "producer = SimpleProducer(client, async=False)\n",
    "\n",
    "# Note that the application is responsible for encoding messages to type bytes\n",
    "producer.send_messages(TOPIC, '1,2017-01-09 11:13:28,2017-01-09 11:25:45,1,3.30,1,N,263,161,1,12.5,0,0.5,2,0,0.3,15.3')\n",
    "\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!sudo {sys.executable} -m pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
